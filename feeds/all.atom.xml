<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jakub Sitnicki's blog</title><link href="http://codecave.cc/" rel="alternate"></link><link href="http://codecave.cc/feeds/all.atom.xml" rel="self"></link><id>http://codecave.cc/</id><updated>2017-08-20T00:00:00+02:00</updated><entry><title>Multipath Routing in Linux - part 2</title><link href="http://codecave.cc/multipath-routing-in-linux-part-2.html" rel="alternate"></link><published>2017-08-20T00:00:00+02:00</published><updated>2017-08-20T00:00:00+02:00</updated><author><name>Jakub Sitnicki &lt;jkbs@redhat.com&gt;</name></author><id>tag:codecave.cc,2017-08-20:/multipath-routing-in-linux-part-2.html</id><summary type="html"></summary><content type="html">&lt;p&gt;&lt;a class="reference external" href="http://codecave.cc/multipath-routing-in-linux-part-1.html"&gt;Last time&lt;/a&gt; we have looked at what is multipath routing, what are
some potential use-cases for it, and how to set it up. In this post we
explore deeper the multipath routing implementation in the Linux
network routing subsystem and focus on the logic behind distributing
packet flows between alternative (equal cost) paths.&lt;/p&gt;
&lt;div class="section" id="two-stacks-two-stories"&gt;
&lt;h2&gt;Two stacks, two stories&lt;/h2&gt;
&lt;p&gt;While you might expect that multipath routing in Linux works the same
for IPv4 and IPv6, that is not the case. As we have seen in &lt;a class="reference external" href="http://codecave.cc/multipath-routing-in-linux-part-1.html#history-dive"&gt;&amp;quot;History
dive&amp;quot; in part 1&lt;/a&gt; support for ECMP (Equal-cost Multipath) routing in
ipv4 and ipv6 stacks &lt;a class="footnote-reference" href="#f0" id="id1"&gt;[1]&lt;/a&gt; has been introduced at different times and
by different developers. Hence the implementations differ and this
impacts how multipath routing can be configured and how it operates.&lt;/p&gt;
&lt;p&gt;In the rest of this post I will concentrate solely on the state of
Multipath Routing in Linux v4.11. However, because it is interesting
to see how the code evolved over time, here is a timeline that
attempts to recap the interesting points in the kernel development
that affected multipath routing:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1997-Nov v2.1.68 Initial support for Multipath Routing for
IPv4. Included support for weighted ECMP (which I talk about
later). Next-hop selected in a pseudo-random fashion (&lt;cite&gt;jiffies&lt;/cite&gt;
value used as a random number).&lt;/li&gt;
&lt;li&gt;2012-Sep v3.6 IPv4 routing cache removed, which made Multipath
Routing unusable with connection-oriented protocols like TCP as
different next-hops could be selected for packets belonging to the
same flow. See &lt;a class="reference external" href="https://serverfault.com/questions/696675/multipath-routing-in-post-3-6-kernels"&gt;discussion at ServerFault&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;2013-Feb v3.8 Initial support for Multipath Routing for
IPv6. Packets belonging to one flow always routed toward the same
next-hop thanks to path selection based on the flow hash. Weighted
ECMP is not supported.&lt;/li&gt;
&lt;li&gt;2016-Jan v4.4 Multipath routing for IPv4 switched to flow hash-based
path selection, making connection-oriented protocols usable with
ECMP again.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is also worth mentioning that between v2.6.12 (2005-Jun) and
v2.6.23 (2007-Oct) there has been an experimental cache support for
Multipath Routing but it's not relevant to current (v4.11)
implementation since routing cache has been removed &lt;a class="footnote-reference" href="#f1" id="id2"&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-ecmp-algorithm-mapping-packets-to-next-hops"&gt;
&lt;h2&gt;The ECMP algorithm - mapping packets to next-hops&lt;/h2&gt;
&lt;p&gt;It is important to keep in mind that existing Multipath Routing
implementation in Linux is designed to distribute flows of packets
over multiple paths, not individual packets. Selecting route in a
per-packet manner does not play well with TCP, IP fragments, or Path
MTU Discovery.&lt;/p&gt;
&lt;p&gt;To associate a packet with a flow, the net stack computes a hash over
a subset of packet header fields. The resulting hash value is what
drives the next-hop selection. In Linux v4.11 the selection of fields
depends on the IP protocol version and whether we are forwarding or
routing locally generated packets. The fields that serve as input to
hashing function are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;for forwarded IPv4 packets (L3 hash)&lt;/p&gt;
&lt;pre class="literal-block"&gt;
{ Source Address, Destination Address }
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;for locally generated IPv4 packets (L4 hash) &lt;a class="footnote-reference" href="#f2" id="id3"&gt;[3]&lt;/a&gt;&lt;/p&gt;
&lt;pre class="literal-block"&gt;
{ Source Address, Destination Address, Protocol, Source Port, Destination Port }
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;for forwarded IPv6 packets (L3 hash)&lt;/p&gt;
&lt;pre class="literal-block"&gt;
{ Source Address, Destination Address, Flow Label, Next Header (protocol) }
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;for locally generated IPv6 packets (L4 hash)&lt;/p&gt;
&lt;pre class="literal-block"&gt;
{ Source Address, Destination Address, Flow Label, Next Header (protocol), Source Port, Destination Port }
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, with recently released Linux v4.12 selection of fields has
changed a bit for IPv4 &lt;a class="footnote-reference" href="#f3" id="id4"&gt;[4]&lt;/a&gt;. An L3 hash is used by default for both
forwarded and locally generated traffic, but the user can choose to
use the L4 hash, in both forward and local output path, with a new
sysctl - &lt;tt class="docutils literal"&gt;net.ipv4.fib_multipath_hash_policy&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;How this flow-hash value is mapped to one of the available next-hops
is where the Linux ipv4 and ipv6 stacks differ. The ipv4 stack
implements a so called Hash-Threshold algorithm which splits all
possible hash values into adjacent ranges, each one corresponding to
one of the available next-hops. The ipv6 stack, on the other hand,
employs a somewhat simpler Modulo-N algorithm, which computes the
next-hop index from the hash value by means of modulo division.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;object data="http://codecave.cc/figures/next-hop-algo.svg" type="image/svg+xml"&gt;
&lt;/object&gt;
&lt;p class="caption"&gt;Figure 1. Next-hop mapping with hash-threshold vs module-N algorithm&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;These mapping schemes are implemented by &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/net/ipv4/fib_semantics.c#L1620"&gt;fib_select_multipath()&lt;/a&gt; and
&lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/net/ipv6/route.c#L462"&gt;rt6_multipath_select()&lt;/a&gt; routines for ipv4 and ipv6, respectively.&lt;/p&gt;
&lt;p&gt;With some setup and a bit of pointer-chasing in &lt;tt class="docutils literal"&gt;gdb&lt;/tt&gt; we can see
&lt;a class="footnote-reference" href="#f4" id="id5"&gt;[5]&lt;/a&gt; how Linux ipv4 stack splits up the hash value range
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;{0..INT_MAX}&lt;/span&gt;&lt;/tt&gt; between available next-hops. Because the ranges are
adjacent we only need to keep track of the upper bound.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# for i in {1..3}; do
&amp;gt;   ip link add test$i type dummy
&amp;gt;   ip link set dev test$i up
&amp;gt;   ip addr add dev test$i 192.168.$i.2/24
&amp;gt; done
# ip route add 2.2.2.2/32 \
&amp;gt;   nexthop via 192.168.1.1 \
&amp;gt;   nexthop via 192.168.2.1 \
&amp;gt;   nexthop via 192.168.3.1
# ip route show 2.2.2.2
2.2.2.2
        nexthop via 192.168.1.1 dev test1 weight 1
        nexthop via 192.168.2.1 dev test2 weight 1
        nexthop via 192.168.3.1 dev test3 weight 1
# gdb gdb /usr/lib/debug/usr/lib/modules/`uname -r`/vmlinux /proc/kcore
...
(gdb) p *(struct fib_info *)fib_info_hash[14].first
$41 = {
  ...
  fib_nhs = 3,
  ...
  fib_nh = 0xffff88003d1d9070
}
(gdb) set $fib_info = (struct fib_info *)fib_info_hash[14].first
(gdb) p $fib_info.fib_nh[0].nh_upper_bound
$32 = {
  counter = 715827882  # &amp;lt;-- INT_MAX/3
}
(gdb) p $fib_info.fib_nh[1].nh_upper_bound
$33 = {
  counter = 1431655764 # &amp;lt;-- INT_MAX/3*2
}
(gdb) p $fib_info.fib_nh[2].nh_upper_bound
$34 = {
  counter = 2147483647 # &amp;lt;-- INT_MAX
}
(gdb)
&lt;/pre&gt;
&lt;p&gt;Both of the algorithms, Hash-Threshold and Modulo-N, are
deterministic, in a sense that the same next-hop is always selected
for the same flow, as long as the set of next-hops has not changed in
the meantime. This is something that is desired in practice, enabling
the same client to always talk to the same server.&lt;/p&gt;
&lt;p&gt;It is also worth noting that the Hash-Threshold and the Modulo-N
algorithms differ in how disruptive &lt;a class="footnote-reference" href="#f7" id="id6"&gt;[6]&lt;/a&gt; they are to the packet
flows when the set of next-hops changes. In this regard, the Modulo-N
algorithm used by the ipv6 stack is more disruptive, that is a larger
percentage of potential flows is affected (re-routed) when a next-hop
is added to or removed from the set of available paths.&lt;/p&gt;
&lt;p&gt;The algorithms are described and analyzed in more detail in:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://tools.ietf.org/html/rfc2991"&gt;RFC 2991&lt;/a&gt; - Multipath Issues in Unicast and Multicast Next-Hop Selection&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://tools.ietf.org/html/rfc2992"&gt;RFC 2992&lt;/a&gt; - Analysis of an Equal-Cost Multi-Path Algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="weighted-ecmp-promoting-selected-next-hops"&gt;
&lt;h2&gt;Weighted ECMP - promoting selected next-hops&lt;/h2&gt;
&lt;p&gt;Weighted Equal-Cost Multi-Path routing is an extension to the concept
of multipath routing where additionally every next-hop has an
associated &amp;quot;weight&amp;quot;. The weight affects how big of a fraction of all
possible flows will be routed toward the next-hop.&lt;/p&gt;
&lt;p&gt;Currently (as of Linux v4.12) only the ipv4 stack respects the
next-hop weight and steers more flows toward a path with a higher
weight. Digging into the code, we can see the weight (stored in
&lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/net/ip_fib.h#L83"&gt;nh_weight&lt;/a&gt; in &lt;tt class="docutils literal"&gt;struct fib_nh&lt;/tt&gt;) being taken into account by
&lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/net/ipv4/fib_semantics.c#L572"&gt;fib_rebalance()&lt;/a&gt; routine which computes the upper range limit of a
next-hop (&lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/net/ip_fib.h#L84"&gt;nh_upper_bound&lt;/a&gt;) we looked at earlier.&lt;/p&gt;
&lt;p&gt;To tell the kernel that we want the next-hops to have different
weights we need to pass the &lt;tt class="docutils literal"&gt;weight&lt;/tt&gt; parameter to &lt;tt class="docutils literal"&gt;ip route&lt;/tt&gt;
command. The specified value will be handed to the kernel in
&lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/uapi/linux/rtnetlink.h#L342"&gt;rtnh_hops&lt;/a&gt; attribute of a next-hop data structure.&lt;/p&gt;
&lt;p&gt;For example, if we want 2/3 (or 10/15) of all possible flows to be
routed towards &lt;tt class="docutils literal"&gt;192.168.1.1&lt;/tt&gt; next-hop and the remaining 1/3 (or
5/15) toward &lt;tt class="docutils literal"&gt;192.168.2.1&lt;/tt&gt; we can set up a multipath route with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# ip route add 2.2.2.2/32 \
&amp;gt; nexthop via 192.168.1.1 weight 10 \
&amp;gt; nexthop via 192.168.2.1 weight 5
# ip route show 2.2.2.2
2.2.2.2
        nexthop via 192.168.1.1 dev dum1 weight 10
        nexthop via 192.168.2.1 dev dum2 weight 5
&lt;/pre&gt;
&lt;p&gt;Just as before, we can take a look at &lt;tt class="docutils literal"&gt;fib_info&lt;/tt&gt; structure to see
that the next-hop-mapped hash value ranges have been set up as
expected:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
192.168.1.1 → {0..INT_MAX*2/3}
192.168.2.1 → {INT_MAX*2/3+1..INT_MAX}
&lt;/pre&gt;
&lt;p&gt;Digging in again with &lt;tt class="docutils literal"&gt;gdb&lt;/tt&gt; we see:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
(gdb) set $ip = $fib_info-&amp;gt;fib_nh[0].nh_gw
(gdb) printf &amp;quot;%d.%d.%d.%d\n&amp;quot;, ($ip &amp;amp; 0xff), ($ip &amp;gt;&amp;gt; 8) &amp;amp; 0xff, ($ip &amp;gt;&amp;gt; 16) &amp;amp; 0xff, ($ip &amp;gt;&amp;gt; 24)
192.168.1.1
(gdb) p $fib_info-&amp;gt;fib_nh[0].nh_weight
$14 = 10
(gdb) p $fib_info-&amp;gt;fib_nh[0].nh_upper_bound.counter
$15 = 1431655764 # &amp;lt;-- INT_MAX/3*2

(gdb) set $ip = $fib_info-&amp;gt;fib_nh[1].nh_gw
(gdb) printf &amp;quot;%d.%d.%d.%d\n&amp;quot;, ($ip &amp;amp; 0xff), ($ip &amp;gt;&amp;gt; 8) &amp;amp; 0xff, ($ip &amp;gt;&amp;gt; 16) &amp;amp; 0xff, ($ip &amp;gt;&amp;gt; 24)
192.168.2.1
(gdb) p $fib_info-&amp;gt;fib_nh[1].nh_weight
$16 = 5
(gdb) p $fib_info-&amp;gt;fib_nh[1].nh_upper_bound.counter
$17 = 2147483647 # &amp;lt;-- INT_MAX
&lt;/pre&gt;
&lt;p&gt;Linux ipv6 stack, however, ignores the next-hop weight, passed from
user-space in &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/uapi/linux/rtnetlink.h#L342"&gt;rtnh_hops&lt;/a&gt;. We can confirm this by looking at
&lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/net/ipv6/route.c#L3283"&gt;inet6_rtm_newroute()&lt;/a&gt; invoked to process &lt;tt class="docutils literal"&gt;RTM_NEWROUTE&lt;/tt&gt; message,
which is send to create a new routing entry.&lt;/p&gt;
&lt;p&gt;What &lt;tt class="docutils literal"&gt;inet6_rtm_newroute()&lt;/tt&gt; does is - it calls a helper routine
&lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/net/ipv6/route.c#L2985"&gt;rtm_to_fib6_config()&lt;/a&gt; to extract the pointer to the first &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/uapi/linux/rtnetlink.h#L339"&gt;struct
rtnexthop&lt;/a&gt; from the message and store it &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/net/ip6_fib.h#L53"&gt;fc_mp&lt;/a&gt; field of &lt;tt class="docutils literal"&gt;struct
fib6_config&lt;/tt&gt;. The stored list of &lt;tt class="docutils literal"&gt;rtnexthop&lt;/tt&gt;'s is later processed
by &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/net/ipv6/route.c#L3122"&gt;ip6_route_multipath_add()&lt;/a&gt; which does not access the &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/uapi/linux/rtnetlink.h#L342"&gt;rtnh_hops&lt;/a&gt;
field.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="wrap-up"&gt;
&lt;h2&gt;Wrap up&lt;/h2&gt;
&lt;p&gt;In this post we have explored how Mulipath Routing implementation has
evolved over time in Linux ipv4 and ipv6 stacks. We have also seen how
the two stacks differ in what ECMP algorithm they use to map flows to
next-hops. Lastly we have looked at weighed ECMP setup where some
next-hops are favored more than others and discovered that we it can
be only used with IPv4.&lt;/p&gt;
&lt;p&gt;If you are running Fedora, you can quickly try out all the commands
using the awesome &lt;a class="reference external" href="https://github.com/amluto/virtme"&gt;virtme&lt;/a&gt;
tool. Make sure you have &lt;tt class="docutils literal"&gt;virtme&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;kernel-debuginfo&lt;/span&gt;&lt;/tt&gt; packages
installed. Then simply run:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ virtme-run --installed-kernel -a nokaslr --qemu-opts -m 1G
...
virtme-init: console is ttyS0
bash-4.3# for i in 1 2; do
&amp;gt;         ip link add dum$i type dummy
&amp;gt;         ip link set dev dum$i up
&amp;gt;         ip addr add dev dum$i 192.168.$i.2/24
&amp;gt; done
bash-4.3# ip route add 2.2.2.2/32 \
&amp;gt;    nexthop via 192.168.1.1 weight 10 \
&amp;gt;    nexthop via 192.168.2.1 weight 5
bash-4.3# ip route show 2.2.2.2
2.2.2.2
        nexthop via 192.168.1.1 dev dum1 weight 10
        nexthop via 192.168.2.1 dev dum2 weight 5
bash-4.3# gdb /usr/lib/debug/lib/modules/`uname -r`/vmlinux /proc/kcore
GNU gdb (GDB) Fedora 7.12.1-48.fc25
...
(gdb)
&lt;/pre&gt;
&lt;p class="rubric"&gt;Footnotes&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="f0" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;From here on I will refer to the IPv4 network stack
implementation in Linux that lives under &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/net/ipv4"&gt;net/ipv4/&lt;/a&gt; as
ipv4 (all lower case) to distinguish it from the IPv4
protocol. Same distinction applies to the IPv6 protocol and
the ipv6 stack in Linux (&lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/net/ipv6"&gt;net/ipv6/&lt;/a&gt;).&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I stumbled upon the description of Multipath Routing cache in
&lt;a class="reference external" href="http://amzn.eu/1S08po2"&gt;Understanding Linux Network Internals&lt;/a&gt;,
&lt;cite&gt;&amp;quot;Cache Support for Multipath&amp;quot;&lt;/cite&gt; section if you would like to know
more about it.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;since commit &lt;a class="reference external" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9920e48b830a"&gt;9920e48b830a (&amp;quot;ipv4: use l4 hash for locally
generated multipath flows&amp;quot;)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;see commit &lt;a class="reference external" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=bf4e0a3db97eb882368fd82980b3b1fa0b5b9778"&gt;bf4e0a3db97e (&amp;quot;net: ipv4: add support for ECMP hash
policy choice&amp;quot;)&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[5]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;If all values in &lt;tt class="docutils literal"&gt;gdb&lt;/tt&gt; show up as zeros you are likely
running a kernel with &lt;a class="reference external" href="https://lwn.net/Articles/569635/"&gt;KASLR&lt;/a&gt; enabled. Confirm by
checking if &lt;tt class="docutils literal"&gt;CONFIG_RANDOMIZE_BASE&lt;/tt&gt; build option was set.
You can disable KASLR by passing &lt;tt class="docutils literal"&gt;nokaslr&lt;/tt&gt; boot option to
the kernel.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f7" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id6"&gt;[6]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;In this context, the flow is considered disrupted when the
network path it takes changes.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- Links
Understanding Linux Network Internals
https://serverfault.com/questions/696675/multipath-routing-in-post-3-6-kernels
https://serverfault.com/questions/276760/linux-multipath-routing-load-balance
https://serverfault.com/questions/820269/how-to-achieve-per-packet-multipath-routing-on-linux --&gt;
&lt;/div&gt;
</content></entry><entry><title>Multipath Routing in Linux - part 1</title><link href="http://codecave.cc/multipath-routing-in-linux-part-1.html" rel="alternate"></link><published>2017-06-24T00:00:00+02:00</published><updated>2017-06-24T00:00:00+02:00</updated><author><name>Jakub Sitnicki &lt;jkbs@redhat.com&gt;</name></author><id>tag:codecave.cc,2017-06-24:/multipath-routing-in-linux-part-1.html</id><summary type="html"></summary><content type="html">&lt;p&gt;This blog post takes a look at the current state of Multipath Routing
mechanism, also known as Equal Cost Multipath Routing (ECMP), in the
Linux v4.11 network stack. If you're running a different kernel -
don't worry. Jump to the &lt;a class="reference internal" href="#history-dive"&gt;History dive&lt;/a&gt; digression at the end to find
out.&lt;/p&gt;
&lt;p&gt;With multipath routing you can distribute traffic destined to a single
network over several paths (routes). It is an extension of the concept
of the conventional routing table, where there is just one
network→next hop association (or network→interface association or
both). Instead, as we will see next, you can specify multiple next
hops for one destination &lt;a class="footnote-reference" href="#f1" id="id1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;What are the use cases? Multipath routing can be used to statelessly
load balance the flows at Layer 3 as described in &lt;a class="reference external" href="https://tools.ietf.org/html/rfc7690"&gt;RFC 7690&lt;/a&gt; and
shown in the figure below.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;object data="http://codecave.cc/figures/ecmp-load-balance.svg" type="image/svg+xml"&gt;
&lt;/object&gt;
&lt;p class="caption"&gt;Figure 1. Stateless load balancing of TCP/UDP flows with multipath routing (ECMP) using anycast scheme (&lt;a class="reference external" href="https://tools.ietf.org/html/rfc7690"&gt;RFC 7690&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Or it could provide improved resiliency to failure by having redundant
routes to the end hosts as pictured below.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;object data="http://codecave.cc/figures/ecmp-redundancy.svg" type="image/svg+xml"&gt;
&lt;/object&gt;
&lt;p class="caption"&gt;Figure 2. Redundant routes to the destination using multipath
routing (ECMP) (&lt;a class="reference external" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=a6db4494d218c2e559173661ee972e048dc04fdd"&gt;commit a6db4494d218 &amp;quot;net: ipv4: Consider failed
nexthops in multipath routes&amp;quot;&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;How the packet flows get steered when there is more than one path to
choose from depends on the route configuration and the details of the
implementation in the kernel. Let's take a look at both and highlight
some existing differences between the IPv4 and IPv6 stacks in Linux
kernel.&lt;/p&gt;
&lt;div class="section" id="setting-it-up"&gt;
&lt;h2&gt;Setting it up&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;ip route&lt;/tt&gt; command &lt;a class="footnote-reference" href="#f2" id="id2"&gt;[2]&lt;/a&gt; lets you define a multipath route
with a &lt;tt class="docutils literal"&gt;nexthop&lt;/tt&gt; keyword. For example, to create an IPv6 route to
1001::/64 subnet with two equally-distant next hops, fc00::1 and
fc01::1, run:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
ip route add 1001::/64 \
   nexthop via fc00::1 \
   nexthop via fc01::1
&lt;/pre&gt;
&lt;p&gt;For an IPv4 example, and a demonstration how we can set next hop
preference with weights &lt;a class="footnote-reference" href="#f3" id="id3"&gt;[3]&lt;/a&gt;, refer to the excellent &lt;a class="reference external" href="http://baturin.org/docs/iproute2/#Multipath%20routing"&gt;iproute2 cheat
sheet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Behind the scenes, when you create a multipath route, the &lt;tt class="docutils literal"&gt;ip&lt;/tt&gt;
command uses a &lt;tt class="docutils literal"&gt;rtnetlink&lt;/tt&gt; &lt;a class="footnote-reference" href="#f4" id="id4"&gt;[4]&lt;/a&gt; socket to send an &lt;tt class="docutils literal"&gt;RTM_NEWROUTE&lt;/tt&gt;
message to the kernel. This message contains an &lt;tt class="docutils literal"&gt;RTA_MULTIPATH&lt;/tt&gt;
attribute &lt;a class="footnote-reference" href="#f5" id="id5"&gt;[5]&lt;/a&gt; which is an array of &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/uapi/linux/rtnetlink.h#L330"&gt;struct rtnexthop&lt;/a&gt; records,
each one corresponding to one next hop:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
struct rtnexthop {
      unsigned short          rtnh_len;
      unsigned char           rtnh_flags;
      unsigned char           rtnh_hops;
      int                     rtnh_ifindex;
};
&lt;/pre&gt;
&lt;p&gt;Here is the layout of the complete &lt;tt class="docutils literal"&gt;RTM_NEWROUTE&lt;/tt&gt; message from our
example above with an accompanying hex dump as it gets generated by
the &lt;tt class="docutils literal"&gt;ip route&lt;/tt&gt; command &lt;a class="footnote-reference" href="#f6" id="id6"&gt;[6]&lt;/a&gt;:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;object data="http://codecave.cc/figures/rta_multipath.svg" type="image/svg+xml"&gt;
&lt;/object&gt;
&lt;p class="caption"&gt;Figure 3. Layout of a sample &lt;tt class="docutils literal"&gt;RTM_NEWROUTE&lt;/tt&gt; message for creating
a multipath route specified with the &lt;tt class="docutils literal"&gt;RTA_MULTIPATH&lt;/tt&gt; attribute.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This message is passed to the kernel space handler for
&lt;tt class="docutils literal"&gt;RTM_NEWROUTE&lt;/tt&gt; messages (&lt;tt class="docutils literal"&gt;inet_rtm_newroute()&lt;/tt&gt; or
&lt;tt class="docutils literal"&gt;inet6_rtm_newroute()&lt;/tt&gt;), where it is parsed, validated, and
transformed into an intermediate form (&lt;tt class="docutils literal"&gt;struct fib_config&lt;/tt&gt; or
&lt;tt class="docutils literal"&gt;struct fib6_config&lt;/tt&gt;). From it the actual routing table entry (or
entries in case of IPv6 stack) is built and inserted into the routing
table. These tasks translate into the following data flow through the
call chain on the kernel side for IPv4 routes:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
RTM_NETLINK → struct fib_config → struct fib_info → struct fib_alias

inet_rtm_newroute()
  fib_table_insert()
    fib_create_info()
      fib_insert_alias()
&lt;/pre&gt;
&lt;p&gt;And a slighty different one for IPv6 routes:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
RTM_NETLINK → struct fib6_config → struct rt6_info,
                                   struct rt6_info,
                                   …

inet6_rtm_newroute()
  rtm_to_fib6_config()
  ip6_route_multipath_add()
    fib6_add()
    fib6_add()
    …
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="history-dive"&gt;
&lt;h2&gt;History dive&lt;/h2&gt;
&lt;p&gt;Hash-based multipath routing (where we distribute flows of packets in
contrast to individual packets) has been available in Linux kernel
since v4.4 for IPv4 &lt;a class="footnote-reference" href="#f7" id="id7"&gt;[7]&lt;/a&gt; and since v3.8 for IPv6 &lt;a class="footnote-reference" href="#f8" id="id8"&gt;[8]&lt;/a&gt;. In this
form multipath routing has been also backported and is available in
Red Hat Enterprise Linux 7.3 (RHEL kernel 3.10.0-345.el7 or newer).&lt;/p&gt;
&lt;p&gt;While the &lt;strong&gt;first ever&lt;/strong&gt; support for multipath routing (IPv4 only),
where individual packets were distributed among alternative paths in a
random fashion, has been added to the Linux kernel almost 20 years
ago! That is in &lt;a class="reference external" href="http://www.oldlinux.org/Linux.old/docs/history/2.1.html"&gt;v2.1.68&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post we have looked at the potential use cases for multipath
routing and also touched on the configuration process. Next time we
will dive into the implementation and see how it differs between IPv4
and IPv6 routing subsystem in Linux.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I would like to thank Phil Sutter for his detailed review and feedback
on the post.&lt;/em&gt;&lt;/p&gt;
&lt;div class="section" id="update-2017-06-27"&gt;
&lt;h3&gt;UPDATE 2017-06-27&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;added &lt;cite&gt;History dive&lt;/cite&gt; section&lt;/li&gt;
&lt;li&gt;fixed grammar &amp;amp; spelling mistakes&lt;/li&gt;
&lt;li&gt;added &amp;quot;thank you&amp;quot; note&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p class="rubric"&gt;Footnotes&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="f1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;p class="first"&gt;Actually each next hop, or multiple next hops in ECMP case,
are associated with a &lt;tt class="docutils literal"&gt;(destination, cost/metric)&lt;/tt&gt;
pair. There has even been a &lt;a class="reference external" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=67e194007be08d071294456274dd53e0a04fdf90"&gt;recent fix&lt;/a&gt; in this area.&lt;/p&gt;
&lt;p class="last"&gt;The route cost/metric may optionally be specified with
&lt;tt class="docutils literal"&gt;metric&lt;/tt&gt; keyword passed to the &lt;tt class="docutils literal"&gt;ip route&lt;/tt&gt; command. For an
example see section on &lt;a class="reference external" href="http://baturin.org/docs/iproute2/#Routes%20with%20different%20metric"&gt;&amp;quot;Routes with different metric&amp;quot;&lt;/a&gt; in
the &lt;em&gt;iproute2 cheat sheet&lt;/em&gt;. The kernel stores the metric
value in &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/net/ip_fib.h#L116"&gt;fib_priority&lt;/a&gt; field from &lt;tt class="docutils literal"&gt;struct fib_info&lt;/tt&gt; for
IPv4 routes, and &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/net/ip6_fib.h#L133"&gt;rt6i_metric&lt;/a&gt; field from &lt;tt class="docutils literal"&gt;struct
rt6_info&lt;/tt&gt; for IPv6 routes.&lt;/p&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;See man page for &lt;a class="reference external" href="https://manpages.debian.org/jessie/iproute2/ip-route.8.en.html"&gt;ip-route(8)&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;As we'll later discover setting next-hop weight only works
with IPv4 routes.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;See man page for &lt;a class="reference external" href="http://man7.org/linux/man-pages/man7/rtnetlink.7.html"&gt;rtnetlink(7)&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[5]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;tt class="docutils literal"&gt;rtnetlink&lt;/tt&gt; attributes are encoded in Legth-Type-Value
format and can be nested. See &lt;a class="reference external" href="http://elixir.free-electrons.com/linux/v4.11/source/include/uapi/linux/rtnetlink.h#L155"&gt;struct rtattr&lt;/a&gt; declaration.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id6"&gt;[6]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;p class="first"&gt;You can sniff and capture &lt;tt class="docutils literal"&gt;netlink&lt;/tt&gt; packets using the
netlink monitor device:&lt;/p&gt;
&lt;pre class="last literal-block"&gt;
ip li add mon0 type nlmon
ip li set dev mon0 up
tcpdump -i mon0 -w netlink.pcap
&lt;/pre&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f7" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id7"&gt;[7]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;See commit &lt;a class="reference external" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=0e884c78ee19e902f300ed147083c28a0c6302f0"&gt;0e884c78ee19 &amp;quot;ipv4: L3 hash-based multipath&amp;quot;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="f8" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id8"&gt;[8]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;See commit &lt;a class="reference external" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=51ebd3181572af8d5076808dab2682d800f6da5d"&gt;51ebd3181572 &amp;quot;ipv6: add support of equal cost multipath (ECMP)&amp;quot;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</content></entry><entry><title>Feeding packets to a VM with Scapy</title><link href="http://codecave.cc/feeding-packets-to-a-vm-with-scapy.html" rel="alternate"></link><published>2016-10-22T00:00:00+02:00</published><updated>2016-10-22T00:00:00+02:00</updated><author><name>Jakub Sitnicki &lt;jkbs@redhat.com&gt;</name></author><id>tag:codecave.cc,2016-10-22:/feeding-packets-to-a-vm-with-scapy.html</id><summary type="html"></summary><content type="html">&lt;p&gt;Want to send hand crafted packets to a VM? &lt;a class="reference external" href="http://www.secdev.org/projects/scapy/doc/"&gt;Scapy&lt;/a&gt; can be used for
that. It can sniff on a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/TUN/TAP"&gt;TAP&lt;/a&gt; device, a virtual link between the host
and the VM, and send packets in reaction to what it sees coming from
the VM.&lt;/p&gt;
&lt;p&gt;In this case, I needed to simulate a quirky switch which is notorious
for tagging every packet forwarded to the host with VLAN ID 0.&lt;/p&gt;
&lt;p&gt;This was confusing the network stack in the VM and a simple ping test
from the VM to the outside was failing.&lt;/p&gt;
&lt;p&gt;To reproduce this scenario, I've extended a bit the &lt;a class="reference external" href="http://www.secdev.org/projects/scapy/doc/usage.html#simplistic-arp-monitor"&gt;Simplistic ARP
Monitor&lt;/a&gt; example from Scapy's excellent documentation.&lt;/p&gt;
&lt;p&gt;What we want to do is:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;for every ARP request from the VM, send a VLAN 0 tagged ARP reply,&lt;/li&gt;
&lt;li&gt;for every ICMP Echo request from the VM, send a VLAN 0 tagged ICMP
Echo reply,&lt;/li&gt;
&lt;li&gt;ignore everything else.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My Scapy script to do just that is &lt;a class="reference external" href="https://github.com/jsitnicki/tools/blob/master/net/scapy/arp_and_icmp_reply_with_vlan0.py"&gt;here&lt;/a&gt;. Now all that is left is to
attach the script to a TAP device linking to the VM.&lt;/p&gt;
&lt;p&gt;I like to use Andrew Lutomirski's &lt;a class="reference external" href="https://github.com/amluto/virtme"&gt;virtme&lt;/a&gt; tool to spin up toy VMs
but it's doesn't matter what you use - qemu, virsh,
virt-manager... What is important is the vNIC model you choose. For
example, I had problems with virtio_net v1.0.0 driver
(qemu-system-x86-2.6.2-2.fc24.x86_64), which seems to be filtering
VLAN 0 tagged frames when the device is not in promiscuous mode. A
bug?&lt;/p&gt;
&lt;p&gt;Let's start up a VM:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ sudo virtme-run \
    --installed-kernel \
    --qemu-opts -net nic,model=e1000 -net tap,script=no,downscript=no
[    0.000000] Linux version 4.7.7-200.fc24.x86_64 (mockbuild&amp;#64;bkernel01.phx2.fedoraproject.org) (gcc version 6.2.1 20160916 (Red Hat 6.2.1-2) (GCC) ) #1 SMP Sat Oct 8 00:21:59 UTC 2016
…
virtme-init: console is ttyS0
bash-4.3# ip link show
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: ens2: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 52:54:00:12:34:56 brd ff:ff:ff:ff:ff:ff
bash-4.3# ethtool -i ens2
driver: e1000
version: 7.3.21-k8-NAPI
…
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;e1000&lt;/tt&gt; virtual network device is there. Let's assign it an
address and bring it up:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
bash-4.3# ip address add dev ens2 10.1.1.1/24
bash-4.3# ip link set dev ens2 up
&lt;/pre&gt;
&lt;p&gt;While on the host a TAP device has showed up (you might find it under
another name, like &lt;tt class="docutils literal"&gt;vnetX&lt;/tt&gt;). Let's bring it up. We don't need to do
anything else with it for this test, like enslave it to a bridge.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ ip link show
...
12: tap0: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 6a:fb:0b:61:63:94 brd ff:ff:ff:ff:ff:ff
$ sudo ip link set dev tap0 up
&lt;/pre&gt;
&lt;p&gt;We are ready to carry out the test. Let's put Scapy to work:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ sudo ~/tools/net/scapy/arp_and_icmp_reply_with_vlan0.py tap0
WARNING: No route found for IPv6 destination :: (no default route?)
&lt;/pre&gt;
&lt;p&gt;It will be useful to monitor the traffic exchanged with the VM to
confirm that what is happening is what we expect. On another terminal
run &lt;tt class="docutils literal"&gt;tcpdump &lt;span class="pre"&gt;-n&lt;/span&gt; &lt;span class="pre"&gt;-nn&lt;/span&gt; &lt;span class="pre"&gt;-ei&lt;/span&gt; tap0 &lt;span class="pre"&gt;-t&lt;/span&gt;&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Now let's ping from the VM a fake address on the same subnet as VM
thinks its on:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
bash-4.3# ping -c 3 10.1.1.2
PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data.
64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=20.3 ms
64 bytes from 10.1.1.2: icmp_seq=2 ttl=64 time=8.60 ms
64 bytes from 10.1.1.2: icmp_seq=3 ttl=64 time=7.30 ms

--- 10.1.1.2 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 7.305/12.100/20.394/5.889 ms
&lt;/pre&gt;
&lt;p&gt;Scapy script is reporting what it has sniffed on the TAP interface and
that replies were sent.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
.
Sent 1 packets.
Ether / ARP who has 10.1.1.2 says 10.1.1.1 / Padding
Ether / Dot1Q / ARP is at 0a:e0:c5:28:0c:a7 says 10.1.1.2
.
Sent 1 packets.
Ether / IP / ICMP 10.1.1.1 &amp;gt; 10.1.1.2 echo-request 0 / Raw
Ether / Dot1Q / IP / ICMP 10.1.1.2 &amp;gt; 10.1.1.1 echo-reply 0 / Raw
.
Sent 1 packets.
Ether / IP / ICMP 10.1.1.1 &amp;gt; 10.1.1.2 echo-request 0 / Raw
Ether / Dot1Q / IP / ICMP 10.1.1.2 &amp;gt; 10.1.1.1 echo-reply 0 / Raw
.
Sent 1 packets.
Ether / IP / ICMP 10.1.1.1 &amp;gt; 10.1.1.2 echo-request 0 / Raw
Ether / Dot1Q / IP / ICMP 10.1.1.2 &amp;gt; 10.1.1.1 echo-reply 0 / Raw
&lt;/pre&gt;
&lt;p&gt;And the traffic capture confirms it:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
52:54:00:12:34:56 &amp;gt; ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 60: Request who-has 10.1.1.2 tell 10.1.1.1, length 46
0a:e0:c5:28:0c:a7 &amp;gt; 52:54:00:12:34:56, ethertype 802.1Q (0x8100), length 46: vlan 0, p 0, ethertype ARP, Reply 10.1.1.2 is-at 0a:e0:c5:28:0c:a7, length 28
52:54:00:12:34:56 &amp;gt; 0a:e0:c5:28:0c:a7, ethertype IPv4 (0x0800), length 98: 10.1.1.1 &amp;gt; 10.1.1.2: ICMP echo request, id 246, seq 1, length 64
0a:e0:c5:28:0c:a7 &amp;gt; 52:54:00:12:34:56, ethertype 802.1Q (0x8100), length 102: vlan 0, p 0, ethertype IPv4, 10.1.1.2 &amp;gt; 10.1.1.1: ICMP echo reply, id 246, seq 1, length 64
52:54:00:12:34:56 &amp;gt; 0a:e0:c5:28:0c:a7, ethertype IPv4 (0x0800), length 98: 10.1.1.1 &amp;gt; 10.1.1.2: ICMP echo request, id 246, seq 2, length 64
0a:e0:c5:28:0c:a7 &amp;gt; 52:54:00:12:34:56, ethertype 802.1Q (0x8100), length 102: vlan 0, p 0, ethertype IPv4, 10.1.1.2 &amp;gt; 10.1.1.1: ICMP echo reply, id 246, seq 2, length 64
52:54:00:12:34:56 &amp;gt; 0a:e0:c5:28:0c:a7, ethertype IPv4 (0x0800), length 98: 10.1.1.1 &amp;gt; 10.1.1.2: ICMP echo request, id 246, seq 3, length 64
0a:e0:c5:28:0c:a7 &amp;gt; 52:54:00:12:34:56, ethertype 802.1Q (0x8100), length 102: vlan 0, p 0, ethertype IPv4, 10.1.1.2 &amp;gt; 10.1.1.1: ICMP echo reply, id 246, seq 3, length 64
&lt;/pre&gt;
</content></entry></feed>